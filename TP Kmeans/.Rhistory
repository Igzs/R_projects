install.packages("ggplot2")
install.packages("ggplot2")
remove.packages("ggplot2")
find.package("ggplot2")
find.package("ggplot2",lib.loc=TRUE)
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
sessionInfo()
library("ggplot2")
sessionInfo()
g+geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(size=2,method="lm",se=FALSE,color="black")+labs(x="Displacement",y="Highway Mileage",title="Swirl Rules!")
shiny::runApp('C:/Users/igorf/OneDrive/ING5/DATA ANALYTICS/TP/Shiny_app')
install.packages("ggmosaic")
runApp('C:/Users/igorf/OneDrive/ING5/DATA ANALYTICS/TP/Shiny_app')
runApp('C:/Users/igorf/OneDrive/ING5/DATA ANALYTICS/TP/Shiny_app')
runApp('C:/Users/igorf/OneDrive/ING5/DATA ANALYTICS/TP/Shiny_app')
runApp('C:/Users/igorf/OneDrive/ING5/DATA ANALYTICS/TP/Shiny_app')
bp<- ggplot(df, aes(x="", y=value, fill=group))+
geom_bar(width = 1, stat = "identity")
pie <- bp + coord_polar("y", start=0)
bp<- ggplot(df_ucba, aes(x="", y=Admit, fill=Gender))+
geom_bar(width = 1, stat = "identity")
data("UCBAdmissions")
df_ucba <- tbl_df(UCBAdmissions)
bp<- ggplot(df_ucba, aes(x="", y=Admit, fill=Gender))+
geom_bar(width = 1, stat = "identity")
bp
pie <- bp + coord_polar("y", start=0)
pie
bp<- ggplot(df_ucba %>% filter(Gender=="Male"), aes(x="", y=Admit, fill=Gender))+
geom_bar(width = 1, stat = "identity")
pie
pie <- bp + coord_polar("y", start=0)
pie
bp<- ggplot(df_ucba %>% filter(Gender=="Male"), aes(x="", y=Gender, fill=Admit))+
geom_bar(width = 1, stat = "identity")
pie <- bp + coord_polar("y", start=0)
pie
bp<- ggplot(df_ucba %>% filter(Gender=="Male"), aes(x=n, y=Gender, fill=Admit))+
geom_bar(width = 1, stat = "identity")
pie <- bp + coord_polar("y", start=0)
pie
bp<- ggplot(df_ucba %>% filter(Gender=="Male"), aes(x=n, y=Gender, fill=Admit))+
geom_bar(width = 1, stat = "identity") + coord_polar("y",start=0)
bp
pie <- ggplot(df_ucba %>% filter(Gender=="Male"), aes(x=n, y=Gender, fill=Admit))+
geom_bar(width = 1, stat = "identity") + coord_polar("y",start=0)
pie
pie <- ggplot(df_ucba %>% filter(Gender=="Male"), aes(x="", y=Gender, fill=Admit))+
geom_bar(width = 1, stat = "identity") + coord_polar("y",start=0)
pie
pie <- ggplot(df_ucba %>% filter(Gender=="Male"), aes(x=factor(1), y=Gender, fill=Admit))+
geom_bar(width = 1, stat = "identity") + coord_polar("y",start=0)
pie
df_plot <- df_ucba %>% filter(Gender=="Male") %>% group_by(Gender)
df_plot
df_plot <- df_ucba %>% filter(Gender=="Male") %>% group_by(Admit)
df_plot
df_plot <- df_ucba %>% filter(Gender=="Male") %>% group_by(Admit,Gender)
df_plot
df_plot <- df_ucba %>% filter(Gender=="Male") %>% group_by(Admit,Gender,DEpt)
df_plot <- df_ucba %>% filter(Gender=="Male") %>% group_by(Admit,Gender,Dept)
df_plot
df_plot <- df_ucba %>% group_by(Admit,Gender,Dept)
df_plot
df_plot <- df_ucba %>% group_by(Admit)
df_plot
df_ucba
df_ucba %>% group_by(Gender)
df_ucba %>% group_by(Admit)
df_ucba %>% group_by(admit)
df_ucba %>% group_by(Admit)
df_ucba %>% group_by(Admit,Gender,Dept)
df_ucba %>% group_by(Dept)
library(dplyr)
df_ucba %>% group_by(Dept)
df_ucba %>% group_by(Dept,Admit)
df_plot <- df_ucba %>% group_by(Gender)
df_plot
df_plot <- df_ucba %>% group_by(Admit)
df_plot
detach(package:plyr)
df_plot <- df_ucba %>% dplyr::group_by(Admit)
df_plot
packageVersion("dplyr")
type(df_ucba)
head(df_ucba)
g <- group_by(Admit)
g <- group_by(df_ucba,Admit)
group_data(g)
g <- group_by(df_ucba,Admit,Gender)
group_data(g)
df_plot <- df_ucba %>% group_by(Admit,Gender)
df_plot
shiny::runApp('C:/Users/igorf/OneDrive/ING5/DATA ANALYTICS/TP/Shiny_app')
runApp('C:/Users/igorf/OneDrive/ING5/DATA ANALYTICS/TP/Shiny_app')
ggplot(data = df_ucba) +
geom_mosaic(aes(weight= n, x = product(Dept,Admit), fill=Admit)) +
facet_grid(Gender~.) +
scale_fill_manual(values=c("#56B4E9", "#D46A6A"))
df_ucba <- tbl_df(UCBAdmissions)
ggplot(data = df_ucba) +
geom_mosaic(aes(weight= n, x = product(Dept,Admit), fill=Admit)) +
facet_grid(Gender~.) +
scale_fill_manual(values=c("#56B4E9", "#D46A6A"))
X
X= as.data.frame(cbind(c(1, 1, 0, 5, 6, 4),c(4, 3, 4, 1, 2, 0)),col.names=c("X1","X2"))
plot(X[,1],X[,2],xlab="X1",ylab="X2")
X = cbind(X,sample.int(2,1))
X
X[,1]
X[1,]
X[1,][0]
X[1,][1]
X[1,][2]
tmp <- cbind(c(1, 1, 0, 5, 6, 4),c(4, 3, 4, 1, 2, 0))
X <-  as.data.frame(tmp,col.names=c("X1","X2"))
X
X <-  as.data.frame(cbind(c(1, 1, 0, 5, 6, 4),c(4, 3, 4, 1, 2, 0)))
plot(X[,1],X[,2],xlab="X1",ylab="X2")
X = cbind(X,sample.int(2,1))
colnames(X) <- c("X1","X2","Cluster")
X
X[-Cluster]
X[,1]
X[1,]
eucliedianDist <- function(X,Y){
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
}
euclidianDist([1,2],[2,3])
c(1,2)
euclidianDist(c(1,2),c(2,3))
euclidianDist(c(1,2),c(2,3))
euclidianDist <- function(X,Y)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
euclidianDist(c(1,2),c(2,3))
euclidianDist(c(1,2),c(2,33))
euclidianDist(c(1,2),c(2,2))
euclidianDist(c(1,2),c(1,2))
euclidianDist(c(1,2),c(0,2))
euclidianDist(c(1,2),c(1,2))
c(0,2)
clusters = cbind(sample.int(2,10))
clusters = sample.int(2,1)
size(X)
len(X)
dim(X)
clusters = cbind(clusters,sample.int(2,1))
clusters
clusters=[]
print(km)
pointCards <- ligue1 %>% select(Points, yellow.cards)
#set seed for reproductible results
set.seed(28112019)
#run kmeans algorithms with 2 clusters and 20 iterations
km <- kmeans(pointCards,centers=2, iter.max = 20)
#import csv file as a dataframe called ligue1
ligue1 <- read.csv("ligue1_17_18.csv", header=T,sep=";",row.names=1)
View(ligue1)
#print the first two lines of dataframe ligue1
head(ligue1,2)
#total number of features
ncol(ligue1)
#create smaller dataset based on the Points and yellow.cards features
pointCards <- ligue1 %>% select(Points, yellow.cards)
#set seed for reproductible results
set.seed(28112019)
#run kmeans algorithms with 2 clusters and 20 iterations
km <- kmeans(pointCards,centers=2, iter.max = 20)
#describe km
print(km)
#coordinates of the centers of the clusters
km$centers
#plot points according to the belongs they belong to
plot(pointCards, col = km$cluster,xlab="Points",ylab="Yellow Cards")
points(km$centers, pch = 4, col="blue")
#rerun kmeans with 3 and 4 clusters
km3 <- kmeans(pointCards,centers=3, iter.max = 20)
#plot new clusters
plot(pointCards, col = km3$cluster,xlab="Points",ylab="Yellow Cards")
points(km3$centers, pch = 4, col="blue")
#rerun kmeans with 3 and 4 clusters
km4 <- kmeans(pointCards,centers=4, iter.max = 20)
#plot new clusters
plot(pointCards, col = km4$cluster,xlab="Points",ylab="Yellow Cards")
points(km4$centers, pch = 4, col="blue")
# 10.
# Visualize the “within groups sum of squares” of the k-means clustering results (use the code in the link above).
mydata <- pointsCards
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
# 11.
# Modify the code of the previous question in order to visualize the ‘between_SS / total_SS’. Interpret the results.
mydata <- pointsCards
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- (kmeans(mydata, centers=i)$betweenss / kmeans(mydata, centers=i)$totss)
plot(1:15, wss, type="b", xlab="Number of Clusters",ylab="between_SS / total_SS")
# The real improvement is between 1 and 2 clusters, so the best number of cluster is 2
#visualise the within groups sum of squares to determine the best number of clusters
wss <- (nrow(pointCards)-1)*sum(apply(pointCards,2,var))
for (i in 2:10)
wss[i] <- sum(km$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
#another way
pamk.best <- pamk(pointCards)
cat("number of clusters estimated by optimum average silhouette width:", pamk.best$nc, "\n")
plot(pam(pointCards, pamk.best$nc))
ligue1_scaled <- as.data.frame(scale(ligue1))
set.seed(28112019)
km.ligue1 <- kmeans(ligue1,centers=3, iter.max = 20)
km.ligue1.scaled <- kmeans(ligue1_scaled,centers=3, iter.max = 20)
X <-  as.data.frame(cbind(c(1, 1, 0, 5, 6, 4),c(4, 3, 4, 1, 2, 0)))
plot(X[,1],X[,2],xlab="X1",ylab="X2")
clusters = cbind(clusters,sample.int(2,1))
colnames(X) <- c("X1","X2","Cluster")
euclidianDist <- function(X,Y)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
km
ligue1 <- read.csv("ligue1_17_18.csv", header=T,sep=";",row.names=1)
setwd("C:/Users/igorf/OneDrive/ING5/DATA ANALYTICS/TP Clustering")
#import csv file as a dataframe called ligue1
ligue1 <- read.csv("ligue1_17_18.csv", header=T,sep=";",row.names=1)
View(ligue1)
#print the first two lines of dataframe ligue1
head(ligue1,2)
#total number of features
ncol(ligue1)
#create smaller dataset based on the Points and yellow.cards features
pointCards <- ligue1 %>% select(Points, yellow.cards)
#set seed for reproductible results
set.seed(28112019)
#run kmeans algorithms with 2 clusters and 20 iterations
km <- kmeans(pointCards,centers=2, iter.max = 20)
#describe km
print(km)
#coordinates of the centers of the clusters
km$centers
#plot points according to the belongs they belong to
plot(pointCards, col = km$cluster,xlab="Points",ylab="Yellow Cards")
points(km$centers, pch = 4, col="blue")
#rerun kmeans with 3 and 4 clusters
km3 <- kmeans(pointCards,centers=3, iter.max = 20)
#plot new clusters
plot(pointCards, col = km3$cluster,xlab="Points",ylab="Yellow Cards")
points(km3$centers, pch = 4, col="blue")
#rerun kmeans with 3 and 4 clusters
km4 <- kmeans(pointCards,centers=4, iter.max = 20)
#plot new clusters
plot(pointCards, col = km4$cluster,xlab="Points",ylab="Yellow Cards")
points(km4$centers, pch = 4, col="blue")
# 10.
# Visualize the “within groups sum of squares” of the k-means clustering results (use the code in the link above).
mydata <- pointsCards
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
# 11.
# Modify the code of the previous question in order to visualize the ‘between_SS / total_SS’. Interpret the results.
mydata <- pointsCards
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- (kmeans(mydata, centers=i)$betweenss / kmeans(mydata, centers=i)$totss)
plot(1:15, wss, type="b", xlab="Number of Clusters",ylab="between_SS / total_SS")
# The real improvement is between 1 and 2 clusters, so the best number of cluster is 2
#visualise the within groups sum of squares to determine the best number of clusters
wss <- (nrow(pointCards)-1)*sum(apply(pointCards,2,var))
for (i in 2:10)
wss[i] <- sum(km$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
#another way
pamk.best <- pamk(pointCards)
cat("number of clusters estimated by optimum average silhouette width:", pamk.best$nc, "\n")
plot(pam(pointCards, pamk.best$nc))
ligue1_scaled <- as.data.frame(scale(ligue1))
set.seed(28112019)
km.ligue1 <- kmeans(ligue1,centers=3, iter.max = 20)
km.ligue1.scaled <- kmeans(ligue1_scaled,centers=3, iter.max = 20)
X <-  as.data.frame(cbind(c(1, 1, 0, 5, 6, 4),c(4, 3, 4, 1, 2, 0)))
plot(X[,1],X[,2],xlab="X1",ylab="X2")
clusters = cbind(clusters,sample.int(2,1))
colnames(X) <- c("X1","X2","Cluster")
euclidianDist <- function(X,Y)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
km
pointCards <- ligue1 %>% select(Points, yellow.cards)
library(dplyr)
library(dplyr)
#import csv file as a dataframe called ligue1
ligue1 <- read.csv("ligue1_17_18.csv", header=T,sep=";",row.names=1)
View(ligue1)
#print the first two lines of dataframe ligue1
head(ligue1,2)
#total number of features
ncol(ligue1)
#create smaller dataset based on the Points and yellow.cards features
pointCards <- ligue1 %>% select(Points, yellow.cards)
#set seed for reproductible results
set.seed(28112019)
#run kmeans algorithms with 2 clusters and 20 iterations
km <- kmeans(pointCards,centers=2, iter.max = 20)
#describe km
print(km)
#coordinates of the centers of the clusters
km$centers
#plot points according to the belongs they belong to
plot(pointCards, col = km$cluster,xlab="Points",ylab="Yellow Cards")
points(km$centers, pch = 4, col="blue")
#rerun kmeans with 3 and 4 clusters
km3 <- kmeans(pointCards,centers=3, iter.max = 20)
#plot new clusters
plot(pointCards, col = km3$cluster,xlab="Points",ylab="Yellow Cards")
points(km3$centers, pch = 4, col="blue")
#rerun kmeans with 3 and 4 clusters
km4 <- kmeans(pointCards,centers=4, iter.max = 20)
#plot new clusters
plot(pointCards, col = km4$cluster,xlab="Points",ylab="Yellow Cards")
points(km4$centers, pch = 4, col="blue")
# 10.
# Visualize the “within groups sum of squares” of the k-means clustering results (use the code in the link above).
mydata <- pointsCards
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
# 11.
# Modify the code of the previous question in order to visualize the ‘between_SS / total_SS’. Interpret the results.
mydata <- pointsCards
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- (kmeans(mydata, centers=i)$betweenss / kmeans(mydata, centers=i)$totss)
plot(1:15, wss, type="b", xlab="Number of Clusters",ylab="between_SS / total_SS")
# The real improvement is between 1 and 2 clusters, so the best number of cluster is 2
#visualise the within groups sum of squares to determine the best number of clusters
wss <- (nrow(pointCards)-1)*sum(apply(pointCards,2,var))
for (i in 2:10)
wss[i] <- sum(km$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
#another way
pamk.best <- pamk(pointCards)
cat("number of clusters estimated by optimum average silhouette width:", pamk.best$nc, "\n")
plot(pam(pointCards, pamk.best$nc))
ligue1_scaled <- as.data.frame(scale(ligue1))
set.seed(28112019)
km.ligue1 <- kmeans(ligue1,centers=3, iter.max = 20)
km.ligue1.scaled <- kmeans(ligue1_scaled,centers=3, iter.max = 20)
X <-  as.data.frame(cbind(c(1, 1, 0, 5, 6, 4),c(4, 3, 4, 1, 2, 0)))
plot(X[,1],X[,2],xlab="X1",ylab="X2")
clusters = cbind(clusters,sample.int(2,1))
colnames(X) <- c("X1","X2","Cluster")
euclidianDist <- function(X,Y)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
km
km$size
km
euclidianDist <- function(X,Y)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
X
X <-  as.data.frame(cbind(c(1, 1, 0, 5, 6, 4),c(4, 3, 4, 1, 2, 0)))
X
euclidianDist(X[1,],X[2,])
euclidianDist(X[1,],X[3,])
euclidianDist <- function(X,Y)
print(X,Y)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
euclidianDist <- function(X,Y){
print(X,Y)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
}
euclidianDist(X[1,],X[3,])
euclidianDist <- function(X,Y){
print(X)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
}
euclidianDist(X[1,],X[3,])
X[3,]
euclidianDist <- function(X,Y){
print(X)
print(Y)
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
}
euclidianDist(X[1,],X[3,])
2**2
2^2
euclidianDist <- function(X,Y){
print(X[1])
print(Y[1])
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
}
euclidianDist(X[1,],X[3,])
euclidianDist <- function(X,Y){
print(X[1]-X[2])
print(Y[1]-Y[2])
sqrt((X[1]-X[2])**2-(Y[1]-Y[2])**2)
}
euclidianDist(X[1,],X[3,])
euclidianDist <- function(X,Y){
print(X[1]-X[2])
print(Y[1]-Y[2])
sqrt((X[1]-X[2])**2+(Y[1]-Y[2])**2)
}
euclidianDist(X[1,],X[3,])
print(km.ligue1)
clusters
clusters <- cbind(clusters,sample.int(2,1))
clusters <- cbind(sample.int(2,1))
clusters <- cbind(sample.int(2,1))
clusters
(sample.int(101,size=100,replace=TRUE)-1)/100
sample.int(2,size=6)
runif
runif(6,1,2)
round(unif(6,1,2))
round(runif(6,1,2))
dim(X)
dim(X)[1]
clusters <- round(runif(dim(X)[1],1,2))
clusters
plot(X[,1],X[,2],xlab="X1",ylab="X2")
runif(1,0,6)
runif(1,0,6)
runif(1,0,6)
V = [runif(1,0,6)]*2
V = runif(1,0,6)*2
V
V = runif(1,0,6)
V = runif(1,0,6)
V
V = runif(2,0,6)
V
V <- runif(2,0,6)
V
ligue1_scaled <- as.data.frame(scale(ligue1))
plot(X[,1],X[,2],xlab="X1",ylab="X2")
points(V)
plot(X[,1],X[,2],xlab="X1",ylab="X2")
for (i in 0:dim(X)[1])
for(k in 2)
print(i,k)
for (i in 0:dim(X)[1]){
for(k in 2){
print(i,k)
}
}
for (i in 0:dim(X)[1]){
for(k in 2){
print(i,k,"test")
}
}
for (i in 0:dim(X)[1]){
for(k in 2){
print(i)
print(k)
}
}
nb_clusters=2
for (i in 0:dim(X)[1]){
for(k in nb_clusters){
print(i)
print(k)
}
}
for (i in 0:dim(X)[1]){
for(k in nb_clusters){
print(i,k)
}
}
for (i in 0:dim(X)[1]){
for(k in nb_clusters){
print(c(i,k))
}
}
for (i in 0:dim(X)[1]){
for(k in 1:2){
print(c(i,k))
}
}
for (i in 1:dim(X)[1]){
for(k in 1:2){
print(c(i,k))
D[i][k] = euclidianDist(X[i,],V[k])
}
}
V[1]
V[2]
X[1,]
X[6,]
for (i in 1:dim(X)[1]){
for(k in 1:2){
print(c(i,k))
D[i][k] = euclidianDist(X[i,],V[k])
}
}
for (i in 1:dim(X)[1]){
for(k in 1:2){
print(c(i,k))
D[i][k] <- euclidianDist(X[i,],V[k])
}
}
D = mmatrix()
D = matrix()
D
D[1][1]
D[1][1] <- 2
D
nb_clusters=2
D = matrix()
for (i in 1:dim(X)[1]){
for(k in 1:2){
print(c(i,k))
D[i][k] <- euclidianDist(X[i,],V[k])
}
}
D
