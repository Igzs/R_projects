---
title: "Week 8"
subtitle: "Hierarchical Clustering"
author: Igor FIDALGO
date: "`r format(Sys.time())`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercice 1
Download the iris dataset and import it into R.

```{r iris,  message=FALSE}
library(dplyr)
data(iris)
iris_df <- tbl_df(iris)
```

## Exercice 2
Choose randomly 40 observations of the iris dataset and store the sample dataset into sampleiris.

```{r sample}
set.seed(2122019)
sampleiris <- sample_n(iris_df,40)

```

## Exercice 3
Calculate the euclidean distances between the flowers. Store the results in a matrix called D. (Remark: the last column of the dataset is the class labels of the flowers)

```{r dist}
sampleiris_num <- sampleiris %>% select(-Species)
sampleiris_scaled <- scale(sampleiris_num)
D <- dist(sampleiris_scaled)
```

## Exercice 4
Construct a dendrogram on the iris dataset using the method average. Store the result in dendro.avg.

```{r }
 dendro.avg <- hclust(D, method = "average")
```

## Exercice 5
Plot the dendrogram.

```{r}
 plot(dendro.avg)
```

## Exercice 6
Plot again the dendrogram using the following command:
```{r}
 plot(dendro.avg, hang=-1, label=sampleiris$Species)
```

## Exercice 7
To cut the dendrogram and obtain a clustering use the cutree. You can choose the number of clusters you wish to obtain, or you can cut by choosing the height from the dendrogram figure. Cut the dendrogram in order to obtain 3 clusters. Store the results into vector groups.avg.

```{r}
groups.avg <- cutree(dendro.avg, k = 3) 
```

## Exercice 8
Visualize the cut tree using the function rect.hclust(). You can choose the colors of the rectangles too!

```{r}
 plot(dendro.avg, hang=-1, label=sampleiris$Species)
 rect.hclust(dendro.avg,k=3, border = c("red","blue","green"))
```

## Exercice 9
Compare the obtained results obtained with Hierarchical clustering and the real class labels of the flowers (function table()). Interpret the results.

```{r  }
table(sampleiris$Species,groups.avg)
```
Using the average linkage method for hierarchical clustering, we can observe that the clusters are correctly separated, except for the Virginica species. 4 out of 13 observations of this group are aggregated in the Versicolor cluster. This can be explained by the similarity of the points in terms of Euclidian distance. 

## Exercice 10

Now apply the Hierarchical clustering on the iris dataset (the 150 observations). Choose 3 clusters and compare the results with the real class labels. Compare different methods of Hierarchical clustering (average, complete and single linkages).

```{r, fig.width=20,fig.height=12 }
 iris_num <- iris_df %>% select(-Species)

iris_scaled <- scale(iris_num)
D <- dist(iris_scaled)

dendro.iris.avg <- hclust(D, method="average")
dendro.iris.comp <- hclust(D, method="complete")
dendro.iris.sgl <- hclust(D, method="single")

plot(dendro.iris.avg, main="Cluster dendogram with average linkage", hang=-1, label=iris_df$Species)
rect.hclust(dendro.iris.avg,k=3, border = c("red","blue","green"))

plot(dendro.iris.comp, main="Cluster dendogram with complete linkage",hang=-1, label=iris_df$Species)
rect.hclust(dendro.iris.comp,k=3, border = c("red","blue","green"))

plot(dendro.iris.sgl, main="Cluster dendogram with single linkage",hang=-1, label=iris_df$Species)
rect.hclust(dendro.iris.sgl,k=3, border = c("red","blue","green"))



groups.iris.avg <- cutree(dendro.iris.avg, k=3)
groups.iris.comp <- cutree(dendro.iris.comp, k=3)
groups.iris.sgl <-cutree(dendro.iris.sgl, k=3)

table(iris_df$Species,groups.iris.avg)
table(iris_df$Species,groups.iris.comp)
table(iris_df$Species,groups.iris.sgl)

```

## Conclusion
We can see that the single and average linkage method give us mixed clusters. They seem heavily biased by outliers values, specially for the single method where the second cluster is composed of juste one value.  

The comple linkage method gives us more distinct clusters, although the third cluster is composed of two Species. From this we can conclude that complete is a more robust clustering method. 